[{
    "title": "Synthetic Dataset using Blender+Python: Part 2",
    "date": "December 20, 2020",
    "description": "Rendering the dataset",
    "body": "\nNow that we are comfortable with the Blender starters, we can start using Python to automate some of its aspects and generate a synthetic dataset. Pre-requisites: I will be using Ubuntu 20.01 and Blender 2.91.\nObjective: To generate a dataset using Blender and Python, with the characteristics:\n Assume our object is centered at the origin (0,0,0) Capture the object from a particular distance (R), in a circular path, a total of 10 images The script should also output camera locations and orientation (in Z axis) along with the frames  Such dataset may help us find out the camera/robot\u0026rsquo;s location given a test image; not so simple as it sounds ;)\nLet\u0026rsquo;s start with setting up our environment.\n Open the console and launch Blender.  $ blender  Start with default project: The default project (create one if you haven\u0026rsquo;t\u0026hellip;) gives you a cube centered at the origin with a Camera and a Light. As discussed in the last part, the object can be replaced with your object of Interest by simply importing its 3D model. For better visualization, I have duplicated the default cube (CTRL+C and CTRL+V) and colored them.\n  Setup the Camera: We plan to take snaps of the object of Interest (OoI) from various points of the trajectory programmed/desired by us. So, we start with an initial setup for our objects: camera, light and all cube(s). It signifies the initial position of our camera, before it can start capturing anything. The Object Properties for Camera look like:\n   Automate Camera motion using Python script: Let\u0026rsquo;s first try to understand what are we trying to accomplish here. Here, I am trying to move the camera in a circular trajectory but only till a quadrant; camera starts at the X axis and ends up at the Y-axis.   For a particular radius, this trajectory is traversed by shifting the camera in small steps. The smaller the steps (or step-size), the better (and more) the data. The same step is repeated for different distances or better called as radii.\n We are familiar with with the fact that the X and Y coordinate in Cartesian coordinate can be replaced with rCos(theta) and rSin(theta) in spherical coordinate system. So, we can first find theta, and for a particular radius (r), we can find x and y. The Setup the Camera section shows the initial camera orientation. The X and Y coordinates have been fixed by us initially. We find the angle made by the camera at this time. Let\u0026rsquo;s call it init_angle. This is very close to the X axis, as obvious. Now, we need to limit our motion to a maximum of 90 degrees (or a single quadrant). Let\u0026rsquo;s call it target_angle. Now, while going from init_angle to target_angle, the number of steps to be taken are specified by num_steps_revolution (just because the camera is revoluting about the origin or the first cube). For simplicity, we choose only a single radius for trajectory.\nLet\u0026rsquo;s not change the lights and get to the code.\nImport required dependencies:\nimport bpy import os import numpy as np from math import * from mathutils import * Now, we define the locations and names of the objects that we will be needing: the target a.k.a the OoI (object of interest):\n#set your own target here target = bpy.data.objects[\u0026#39;Cube\u0026#39;] #Do not forget to check the object name t_loc_x = target.location.x t_loc_y = target.location.y The target object is the one around which we want the camera to face. In our case, its the cube centered at the origin.\ncam = bpy.data.objects[\u0026#39;Camera\u0026#39;] cam_loc_x = cam.location.x cam_loc_y = cam.location.y Now, define the angles and radius.\nR = (target.location.xy-cam.location.xy).length # Radius num_steps_revolution = 36 #how many revolution steps in each circle/revolution #ugly fix to get the initial angle right init_angle = (1-2*bool((cam_loc_y-t_loc_y)\u0026lt;0))*acos((cam_loc_x-t_loc_x)/dist)-2*pi*bool((cam_loc_y-t_loc_y)\u0026lt;0) target_angle = (pi/2 - init_angle) # How much more to go... Start looping the camera\nfor x in range(num_steps_revolution): # Use alpha to locate new camera position alpha = init_angle + (x+1)*target_angle/num_steps # Move camera cam.rotation_euler[2] = pi/2+alpha # *Rotate* it to point to the object cam.location.x = t_loc_x+cos(alpha)*R cam.location.y = t_loc_y+sin(alpha)*R # Save Path for renders file = os.path.join(\u0026#39;/home/renders\u0026#39;, x) # saves filename as the step number bpy.context.scene.render.filepath = file bpy.ops.render.render( write_still=True ) # render The entire code (with slightly different variable names) can be found here:\nRunning the entire code in a console (as shown in Part 1), should render and save 36 images in the path specified. A sample would be:\nblender -b ~/Videos/blender/panel-synthetic-dataset.blend -P ~/Videos/blender/test_synthetic.py  To visualise if the camera trajectory will look like, I modified the initial script as follows:\nI replaced the render part with camera generation. Thanks to this wonderful BlenderExchange Site As we increase the angle, progressing from initial angle init_angle to target angle target_angle, at each step, instead of rendering, I ask Blender to place a new camera at the newly calculated position. The result is as follows:\n The blend file can be used for reference here: 10cams.blend\nExcept for the new cameras all facing towards negative Z axis, as it doesn\u0026rsquo;t affect our purpose, everything looks Good :)\n A Step further: This seemed very simplistic but great to understand how to get the job started. I used an upgraded version of the code, to give me even more data: I added rotation to revolution. Till now, our camera shifted to a new position in the same circular trajectory and took a snapshot and moved ahead. But now, we ask it to take even more snaps at the same exact spot, by rotating about itself. Further, I ask it to not only follow a single radius, but a range of radii; we need to specify the radii (r1, r2,\u0026hellip;) for getting closer or farther from the object. This modified script can also be found here:\n# Run blender -b ~/Videos/blender/panel-synthetic-dataset.blend -P ~/Videos/blender/test_synthetic.py \u0026#34;\u0026#34;\u0026#34; The default pose is X: -7m Y: -1m Z: 1m Rotation_X = 90 degrees Rotation_Z = -90 degrees (a.k.a cam.rotation_euler[2]) \u0026#34;\u0026#34;\u0026#34; import bpy import os import numpy as np from math import * from mathutils import * #set your own target here target = bpy.data.objects[\u0026#39;Shape_IndexedFaceSet.018\u0026#39;] cam = bpy.data.objects[\u0026#39;Camera\u0026#39;] t_loc_x = target.location.x t_loc_y = target.location.y cam_loc_x = cam.location.x cam_loc_y = cam.location.y # The different radii range radius_range = range(7,15) R = (target.location.xy-cam.location.xy).length # Radius num_steps_revolution = 10 #how many revolution steps in each circle/revolution num_steps_rotation = 5 #how many rotation steps at each angle rotation_range_limit = 3 # NOTE ! in degrees init_angle = atan(cam_loc_y/cam_loc_x) #in rad init_angle = init_angle + pi # as in 3rd quadrant target_angle = (1.5*pi -pi/6.0 - init_angle) # Go 270-8 deg more (pi/6 or 30deg removed as no suitable frame can be found there for r in radius_range: for x in range(1, num_steps_revolution): alpha = init_angle + (x)*target_angle/num_steps_revolution lim_min = degrees(alpha)-rotation_range_limit #degrees lim_max = degrees(alpha)+rotation_range_limit #degrees offset = 1.0/num_steps_rotation #degrees for dalpha in np.arange(lim_min, lim_max, offset): #print(f\u0026#39;in r:{r}, and alpha: {alpha}, dalpha:{dalpha}\u0026#39;) print(r) cam.rotation_euler[2] = pi/2 + radians(dalpha) # \u0026#34;\u0026#34;\u0026#34; Use alpha to locate new camera position Use dalpha to rotate it at the obtained position to get more frames \u0026#34;\u0026#34;\u0026#34; cam.location.x = t_loc_x+cos(alpha)*r cam.location.y = t_loc_y+sin(alpha)*r # Define SAVEPATH and output filename file = os.path.join(\u0026#39;renders/\u0026#39;, str(r)+\u0026#39;_\u0026#39;+str(round(dalpha-180,3))+\u0026#39;_\u0026#39;+str(round(cam.location.x, 3))+\u0026#39;_\u0026#39;+str(round(cam.location.y, 3))) #dalpha in degrees # Render bpy.context.scene.render.filepath = file bpy.ops.render.render(write_still=True) \u0026#34;\u0026#34;\u0026#34; # Place Dummy Cameras to visualise all potential calculated positions dalpha = radians(dalpha) # Randomly place the camera on a circle around the object at the same height as the main camera new_camera_pos = Vector((r * cos(dalpha), r * sin(dalpha), cam.location.z)) bpy.ops.object.camera_add(enter_editmode=False, location=new_camera_pos) # Set the new camera as active bpy.context.scene.camera = bpy.context.object \u0026#34;\u0026#34;\u0026#34; This script performs similar camera motions with rotation+revolutions and saves the camera data (location, orientation) as the file name. The process took a minimum of 5 hours on my non-GPU system and generated more thatn two thousand images\n  The memory consumption was as follows:\n ",
    "ref": "/blog/blender-and-python-1/"
  },{
    "title": "Synthetic Dataset using Blender+Python: Part 1",
    "date": "December 8, 2020",
    "description": "Exploring Basic Blender Functionalities",
    "body": "\nThe tutorial explores basic functionalities of Blender that are required for our goal: generating synthetic dataset using Blender+Python. Blender can do pretty much anything related to graphic manipulation, from Sculpting to VFX, Blender has you covered.\nPre-Requisite: Install Blender from official site. I am using 2.91.0. on Ubuntu 20.01.\n$ sudo snap install blender --classic #installs current stable version Now, Launch it by clicking on the icon or using your terminal and hitting blender + Enter (Highly Recommended). Running using Clicks/GUI can lead to the errors not getting logged onto the console. Hence, avoid it. You should see Blender open up. The default project gives the cube centered at the origin.\n It is suggested to replace the cube with your object of interest and place it too at the origin. Replacement is as simple as importing a .stl or VRML file (.wrl) file of your 3D object into Blender. There are many other supported formats to import.\n Views: The design/development process is generally fragmented. A typical blender project involves getting the object right; means the mesh as well as the solid are correct. This is followed by setting an environment and the attributes of the objects (like the color) and finally, the render: speaks for itself. Each stage requires more computation resources than the former. Hence, it is generally avoided to use the render view while developing, except for making sure the output of each step looks as desired. The viewport panel provides the options:\n WireFrame Solid Material Preview Rendered   Rotating the View: The 3D navigation in the workspace is performed by panning using the axes on the top right. Zoom In or Zoom Out is done using Mouse Wheel or by sliding dual fingers on the touchpad.\n The color of the axes should be noticed. RED for X, GREEN for Y and BLUE for Z.\nObjects and their Manipulation: The Scene Collection contains all the objects present in the current scene. Objects include 3D objects (like the default Cube), cameras and lights.\n Each object has its unique properties and can be manipulated. Objects like the Cube can be relocated, colored, etc. To see the effect of any change, one must switch to either Material Preview or Rendered view mode. To manipulate an object, one must select it (using a single click) before tweaking its parameters.\n The Camera depicts what the output render would look like. It must be used wisely. It can be handled as an object and its extrinsics can be tweaked easily.\n For a more direct and accurate visualisation on what the end product would look like, the camera view can be toggled while playing with camera parameters.\n Python Scripting: Blender comes with a Python API. Its a powerful utility and can be used anything otherwise done using the GUI. Scripting can be enabled by selecting the scripting tab. The in-built shell is capable of any other console used to handle Blender.\n All the errors are logged on the terminal used to launch Blender. This may be missing in case the launch is made using GUI.\n Running scripts from external file: It is a cumbersome task to use the in-built editor and the console everytime. Its always a better option to use your favourite editor like Atom or VS Code or Sublime Text. You can always create/edit your scripts how you do with other Python files. To run it using the Blender-Python API, run it as:\n$ blender -b ~/PATH/TO/BLEND/FILE.blend -P ~/PATH/TO/PYTHON/SCRIPT.py For instance,\n$ blender -b ~/Videos/blender/panel-synthetic-dataset.blend -P ~/Videos/blender/test_synthetic.py Rendering Files: The scripts can be used to render files as well. The format of the renders can be set accordingly.\n A simple script like\nimport bpy import os # Select Camera cam = bpy.data.objects[\u0026#39;Camera\u0026#39;] # Define SAVEPATH and output filename file = os.path.join(\u0026#39;output/\u0026#39;, \u0026#39;yoyo\u0026#39;) # Render bpy.context.scene.render.filepath = file bpy.ops.render.render(write_still=True) will render and save what the selected camera sees in the defined SAVEPATH. The directory output/ is created in the present working directory, and can be checked by $ pwd in the terminal.\n That\u0026rsquo;s all you need to get started with you synthetic dataset. Till the next part, Namaste :)\n ",
    "ref": "/blog/blender-and-python-0/"
  },{
    "title": "Dual Booting Thinkpad L13 with Ubuntu 20",
    "date": "August 27, 2020",
    "description": "Step-by-step guide to dual boot with Focal Fossa and my experience.",
    "body": "\nThe article shares my experience of the attempt to dual-boot Ubuntu 20.04 on Lenovo Thinkpad L13 Yoga.\nTL;DR, all went well. I will try giving a step-by-step description. I am working on a new laptop with no personal data, just Windows subsystem. It uses UEFI.\nStep 1: Prepare Ubuntu 20 image  Get Ubuntu 20.04 Focal Fossa. Prepare bootable USB using the Universal USB Installer. The simple instructions can be found on the site while downloading it; simple though.\nStep 2: Turn Off Secure Boot  The first step before installation is to make sure the system BIOS is setup correctly.\n Boot into BIOS by pressing the function F1 key at the ‚ÄúLenovo‚Äù splash screen.    Select the Restart menu tab and set OS Optimized Defaults to Disabled.    Switching the ‚ÄúOS Optimized Defaults‚Äù settings may give a warning message. Select ‚ÄúYes‚Äù to continue to disable OS Optimized Defaults.     Select the Startup menu tab. Pressing F9 function key will allow Legacy and UEFI bootable devices by setting UEFI/Legacy Boot to ‚ÄúBoth‚Äù; otherwise, it will be an unchangeable setting to ‚ÄúUEFI only‚Äù. So, press F9.\n  Press function F10 key to save and exit BIOS setup.\n   Step 3: Try Ubuntu  The step involves trying out Ubuntu, before installing. This saves you from unnecessary effort if any functionality (that you expected to work on Linux) does not work as intended.\nNote: If you experience any issues with the Windows .\n Insert the bootable USB stick. Power on the system and press the F12 function key whenever the following Lenovo splash screen appears. On the boot menu list, select the Linux bootable media (with name of your drive)    In some cases, you will get the following screen,   In my case, I could not see the TRY UBUNTU option. In this case, just go to the Ubuntu option. Don\u0026rsquo;t worry as this too will let you try the Linux flavour. You should see Ubuntu running now, try the Bluetooth, WiFi drivers and whatever you wanna test.\nStep 4: Turn Off Bitlocker  Some Thinkpad systems are protected by Bitlocker, which encrypts the Windoes Subsystem data and this makes things tough for Linux installation. If you ignore this step, the warning you might see is as follows:\n To turn it off, use any of the methods shown in this article on minitool\nStep 5: Install Ubuntu  The steps this time, are exactly the ones followed for trying ubuntu in Step 3. That is,\n  Insert the bootable USB stick. Power on the system and press the F12 function key whenever the following Lenovo splash screen appears. On the boot menu list, select the Linux bootable media (with name of your drive)\n  Select Install Ubuntu or simply Ubuntu on the GRUB menu with the black screen.\n   The following screen should appear.\n Again, select Install Ubuntu.\n On seeing the following screen,   Select Install Ubuntu alongside Windows Boot Manager for dual boot and Erase disk and install Ubuntu for removing Windows completely (not recommended).\n  Follow along and select Keyboard Layout, and TimeZone, if you are prompted to.\n  Now, allocate space to Ubuntu on the following screen:\n   Move the slider (although there is none visible) between the two partitions. I have allocated about 482GB to Ubuntu. On hitting next, you will be prompted with\n Select Continue. Now you see\n Again, Continue.\n Apart from some other information that the OS may ask, you should have nothing more to do. Must see somthing like:   Restart your system on completion. Now, you should be able to dual boot the system while booting :)\n Notes on the hands-on experience WiFi \u0026amp; Bluetooth I tried Ubuntu 18.04 LTS on my new ThinkPad; I have using it with my HP laptop for over an year now. I noticed that the bluetooth worked but the WiFi did not. Although I believe some manual driver installation or some hack would resolve it. But, no such issues with Ubuntu 20 LTS.\nFingerPrint Both the flavors(18 and 20) have no support for the fingerprint sensor yet :(\nStylus: The L13 comes with ThinkPad Pen which works like breeze on Windows (like the fingerprint scanner) but on Linux, I think there is much work to be done yet. I tried using it on LibreOffice Draw and Okular to annotate a PDF. All the smooth lines were dramatically distorted as shown below.\n Overheating This is an issue that will get you worried in the beginning, maybe this is common in all mini-powerhorses. Also, this is mainly due to the cooling fans on the bottom, which becomes a problem when you put your lap or a plane table to interrupt the air exchange.\nTrackPad The trackpad works normal, its just that the mouse pointer seems to stray off slowly. But this is very rare (twice in 8 hours approx.) and requires one to make a swipe on the trackpad.\nI suggest improving your experience by following the simple steps explained in this youtube video:\n   I hope it helps someone risking the dual-boot.\n ",
    "ref": "/blog/ubuntuonl13/"
  },{
    "title": "How to Download Udacity Nanodegree Content",
    "date": "July 17, 2020",
    "description": "Using Udacimak to download almost all the content of your Nanodegree Program",
    "body": "\nThe step-by-step tutorial is based on Udacimak - A Udacity Nanodegree Downloader. It allows you to download all the video and text-related content from the courses that you are enrolled in. It is a CLI-based method. I will use ubuntu-18.04 LTS for demonstration.\nNote: Udacimak was inspired after Udacity announced in October 2018 that students who graduate will no longer have life-time access to the course content.\nPre-requisites:   Install node (from maybe from this link)\n  Install git\n  Install npm\n  As per the source repo, these are enough to get started but believe me, its not so simple. That\u0026rsquo;s why I wrote this.\nStep 1: Install latest version of node Open a terminal and execute\n$ sudo apt update $ sudo apt install build-essential checkinstall libssl-dev $ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.35.1/install.sh | bash Then close the terminal and reopen it.\nNow, re-install nvm as\n$ nvm --version $ nvm ls $ nvm ls-remote The execution of last command looks somewhat like this. Note that I have installed the latest version.\n Now run\n$ nvm install 14.7.0 \nStep 2: Authenticate your Udacity login  There are 3 methods to do so, as mentioned here. I prefer the following method.\n Visit classroom.udacity.com and log in Open the Developer Tools (press Cmd + Opt + J on MacOS or F12 on Windows or Ctrl + Shift + C on ubuntu) Go to Application tab, then Storage \u0026gt; Cookies \u0026gt; https://classroom.udacity.com. Find _jwt key, its Value is the Udacity authentication token.  As demonstrated here:\n Now run\n$ udacimak settoken YOUR_LONG_TOKEN_FROM_JWT_KEY \nStep 3: Download the contents as json  To get the index of your course, enter the dashboard of your Nanodegree Program And the index can be found in the URL : https://classroom.udacity.com/nanodegrees/nd131/dashboard/overview\nnd131 is the answer in my case.\nNow run\n$ udacimak download YOUR_INDEX #nd131 FOR ME You will find a not-so-big file with the name of your course. You must rename it to a better name as it contains whitespaces which often cause trouble. I renamed it to course\n\nStep 4: Install \u0026ldquo;youtube-dl\u0026rdquo;  This is perhaps the most prone issue that udacimak users get into. Install it using\n$ sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl $ sudo chmod a+rx /usr/local/bin/youtube-dl $ sudo ln -s /usr/bin/python3 /usr/local/bin/python \nStep 5: Render the json files  Its time to get the actual content from YouTube and other sources.\n  Create a new directory (say render) where you wish to save the final course content.\n  Now run\n  $ udacimak render course --targetdir render You should see the files (mainly youtube videos and gifs) getting downloaded smoothly. If you notice errors, its probably because you didn\u0026rsquo;t install youtube-dl.\nThat\u0026rsquo;s it ! Happy Learning :D\n ",
    "ref": "/blog/udacimak/"
  },{
    "title": "Intel¬Æ VTune‚Ñ¢ Profiler: Basic Installation",
    "date": "July 17, 2020",
    "description": "How to Get Started with Intel¬Æ VTune‚Ñ¢ Profiler",
    "body": "\nStep 1: Download the VTune Profiler  Get the desired version from the official page HERE\nI used the version 2020.update2. The next steps can be followed as per the guide provided HERE. Open the PDF version for your OS. I am using Ubuntu 18.04.4 LTS. I will mention the steps I followed from it.\n\nStep 2: Install Prerequisites On Ubuntu* or Debian*, install the following libraries:\nsudo apt-get install libgtk-3-0 libasound2 libxss1 libnss3 On Red Hat* Enterprise Linux or CentOS*, install the following libraries:\nsudo yum install gtk3 alsa-lib libXScrnSaver nss On Fedora*, install the following libraries:\nsudo dnf install gtk3 alsa-lib libXScrnSaver nss On SUSE* Linux* Enterprise Server (SLES), install the following libraries:\nsudo zypper install gtk3 libasound2 libXss1 mozilla-nss \nStep 3: Install with \u0026ldquo;Installer GUI\u0026rdquo; Use the following steps to install :\n  Extract the installation package to a writeable directory with the following command:\n$ tar -xzf vtune_profiler_\u0026lt;version\u0026gt;.tar.gz   cd to the directory containing the extracted files.\n  Run the following command to launch the installer:\n$ ./install_GUI.sh Running it using sudo privilage, will install in HOME/opt/intel/. The installer will guide you through the installation process. You can install using the default options or customize your installation.\n  After installation succeeds, run the following command to establish the environment:\nFor bash command interpreter:\n$ source \u0026lt;install-dir\u0026gt;/vtune-vars.sh   \nStep 4: Setup project in VTune Refer to this Udacity tutorial video\n  \nStep 5: Source Environment By default, the \u0026lt; install-dir \u0026gt; is the following:\n  /opt/intel/vtune_profiler_\u0026lt; version \u0026gt; when installed with root permissions;\n  $HOME/intel/vtune_profiler_\u0026lt; version \u0026gt; when installed with user permissions;\n  /opt/intel/inteloneapi/vtune/\u0026lt; version \u0026gt; for Intel oneAPI Base Toolkit installation.\n  Run\n$ source \u0026lt; install-dir \u0026gt;/env/vars.sh \nStep 6: Run VTune Run the GUI using\n$ vtune-gui For more info and on What can You do using VTune Amplifier, refer to the official site.\n Some Advanced concepts in VTune  The advances profiling gives advantages as\n It gives the hardware usage and helps in optimizing the application to utilize all the underlying hardware. It helps in fixing areas of code that are not optimized.  The following tutorial shows how individual threads can be analysed.\n  In the analysis, the colors have the following significance\n   Color Description     Green Running Time   Brown CPU Utilization   Red Spin and Overhead Percentage    The line that took the most CPU Time can be found as\n  ",
    "ref": "/blog/setup_intel_vtune/"
  },{
    "title": "Decoding YOLOv3 output with Intel OpenVINO's backend",
    "date": "June 6, 2020",
    "description": "Explanation on how the YOLOv3 models output can be decoded from a programming POV",
    "body": "Foreword: The article aims at simplifying the process of getting the understandable results from the RAW output of the YOLOv3 models (v3 and v3-tiny). I will be demonstrating the code snippets from the official demo example provided by OpenVINO toolkit that work for both theses versions but I explain only the v3-tiny which can be generalised for the entire v3 family. Also, I strongly suggest you to get a theoritical understanding of the same from the amazing article by Ethan Yanjia Li. I have almost used Ctrl+C and Ctrl+V from his article to cover up theoritical portions.\nBefore diving directly into the code, its important to understand some concepts. I will try to summarise them here so they aid in clarifying the other parts.\nThe YOLOv3 Methodology: YOLO or You Only Look Once is a single shot detector that is not just another stack of CNNs and FC Layers and perhaps, the paper is itself too chill to give all the crucial details.\nArchitecture: The entire system is is divided into two major component: Feature Extractor and Detector, both are multi-scale. When a new image comes in, it goes through the feature extractor first so that we can obtain feature embeddings at three (or more) different scales. Then, these features are feed into three (or more) branches of the detector to get bounding boxes and class information. v3 outputs three feature vectors: (52x52), (26x26) and (13x13) whereas v3-tiny outputs only (26x26) and (13x13).\nAnchor Box: This is something very naive yet amazing. It definitely takes some time to sink in. Read this carefully:\n The goal of object detection is to get a bounding box and its class. Bounding box usually represents in a normalized xmin, ymin, xmax, ymax format. For example, 0.5 xmin and 0.5 ymin mean the top left corner of the box is in the middle of the image. Intuitively, if we want to get a numeric value like 0.5, we are facing a regression problem. We may as well just have the network predict for values and use Mean Square Error to compare with the ground truth. However, due to the large variance of scale and aspect ratio of boxes, researchers found that it‚Äôs really hard for the network to converge if we just use this ‚Äúbrute force‚Äù way to get a bounding box. Hence, in Faster-RCNN paper, the idea of an anchor box is proposed.\n  Anchor box is a prior box that could have different pre-defined aspect ratios (i.e., the authors already have some pre-defined boxes even before the detection begins).\n  These aspect ratios are determined before training by running K-means on the entire dataset. But where does the box anchor to? We need to introduce a new notion called the grid. In the ‚Äúancient‚Äù year of 2013, algorithms detect objects by using a window to slide through the entire image and running image classification on each window. However, this is so inefficient that researchers proposed to use Conv net to calculate the whole image all in once.\n These aspect ratios or width and height of the anchor boxes are given in the .confg files by the authors. They are not normalised unlike most other values. They are arranged as pair of width and height (w1,h1,w2,h2,w3,h3,\u0026hellip;w18,h18) as: pair x 3 anchors x 2 Detector_layers = 18 anchor points (or 9 pairs).\nAnd specifically this last part:\n Since the convolution outputs a square matrix of feature values (like 13x13, 26x26, and 52x52 in YOLO), we define this matrix as a ‚Äúgrid‚Äù and assign anchor boxes to each cell of the grid. In other words, anchor boxes anchor to the grid cells, and they share the same centroid.\n In other words, the authors thought: \u0026ldquo;Instead of predicting the boxes (or rather their location \u0026amp; dimensions) from scratch, lets place some pre-determined boxes, in the regions where objects are probably found (found using K-Means) and then, the ground-truth (or actual) values (of location and dimensions) for these boxes can be calculated by simply finding the offsets to the location and dimensions of the box\u0026rdquo;\nThe two detectors will each be giving a grid of shape:\n   Layer/Detector Grid shape     Conv_12 26x26   Conv_9 13x13    Note: We are specifically talking about YOLOv3-tiny. For the larger YOLOv3, another detector gives a grid of shape 52x52. Both these models accept strictly resized images of shape 416x416x3.\nIf this was image:\n Then the grid over the image, by the Conv_9 layer would be\n Notice that this also implies that within each cell of a grid, objects are detected using these anchors; that is, the maximum number of objects that can be detected within a cell = number of anchor boxes in it. In v3-tiny, each cell has only 3 anchor boxes. So, each grid cell looks somewhat like this:\n What does each cell hold ? Each cell has 3 anchors in v3 and v3-tiny. Each anchor has the following attributes of its location, dimensions, objectness score and class probablities:\n tx: (a single float) x-coordinate of the centroid (of anchor box) relative to the top-left corner of that cell ty: (a single float) y-coordinate of the centroid (of anchor box) relative to the top-left corner of that cell tw: (a single float) absolute width of the bounding box th: (a single float) absolute height of the bounding box confidence: (a single float) The probablity that the anchor box did detect some object. class scores: (80 float values) The 80 classes with their scores. If the confidence is above our preset threshold, we pick the one class out of these 80 classes, with highest value. The result array looks like    Note: I have used x and y instead of tx and ty in the code and the diagrams. Pardon me for that as I am bound to copy-paste while following the demo code from OpenVINO demos.\nTo explain tx and ty, for example, tx = 0.5 and ty = 0.5 means the top left corner of the box is in the middle of the image i.e, the centroid of the detected bounding box is at the exact center of that grid cell and not the entire image. Notice that all three anchor boxes of each cell share a common centroid. The absolute value of these bounding boxes has to be calculated by adding the grid cell location (or its index) to its x and y coordinates. To understand, look at the below figure from the official paper and the example below:\n As an example,\n (In the figure,) Cx and Cy represents the absolute location of the top-left corner of the current grid cell. So if the grid cell is the one in the SECOND row and SECOND column of a grid 13x13, then Cx = 1 and Cy = 1. And if we add this grid cell location with relative centroid location, we will have the absolute centroid location bx = 0.5 + 1 and by = 0.5 + 1. Certainly, the author won‚Äôt bother to tell you that you also need to normalize this by dividing by the grid size, so the true bx would be 1.5/13 = 0.115\n tw and th are normalised too. To get absolute values of width and height, we need to multiply them with their respective anchor width or height and again normalize by the image width or height respectively (fixed to 416x416 for v3 and v3-tiny). But why is it so twisted üòï. To that\u0026hellip; Its like that\n A simplified instruction is:\n Get the normalised tw and th from the detections. Process this value using the exponent or exp function. (As we may get -ve or big values sometimes) Multiply this value with the pre-determined absolute values (aspect ratio) of the anchor box. Again normalise this result using aspect ratio of the resized image(416x416). Use these results as offsets to get x \u0026amp; y coordinates from the coordinates of the centroid with respect to the center of the bounding box.  The code section below will give you more clarity on it.\n What about the code ? Note: It is assumed that the reader accepts that I have used OpenVINO backend just as any other method to fetch results from the model and only aim to focus the decoding part, which is common.\nWe start with the pre-processed frame pframe fed to the inference engine ie. I use the object infer_network of the class Network. Our original image was:\n The pre-processing was done as:\n### Pre-process the image as needed ### b, c, h, w = infer_network.get_input_shape() pframe = cv2.resize(frame,(w,h)) pframe = pframe.transpose((2,0,1)) pframe = pframe.reshape((b,c,h,w)) Let\u0026rsquo;s jump directly into the raw output of the inference. The output is obtained as output = self.exec_net.requests[0].outputs. This is a dictionary with 2x{Layer, feature_map_values}.\nfor layer_name, out_blob in output.items(): print(out_blob.shape) print(\u0026quot;Layer:{}\\nOutBlob:{}\u0026quot;.format(layer_name, out_blob)) #Layer | Feature map shape #detector/yolo-v3-tiny/Conv_12/BiasAdd/YoloRegion | (1, 255, 26, 26) #detector/yolo-v3-tiny/Conv_9/BiasAdd/YoloRegion | (1, 255, 13, 13)  Wait, first Tell me something about the model  Originally, YOLOv3 model includes feature extractor called Darknet-53 with three branches for v3 (and 2 branches for v3-tiny) at the end that make detections at three different scales. These branches must end with the YOLO Region layer. (named as simply YOLO) Region layer was first introduced in the DarkNet framework. Other frameworks, including TensorFlow, do not have the Region implemented as a single layer, so every author of public YOLOv3 model creates it using simple layers. This badly affects performance. For this reason, the main idea of YOLOv3 model conversion to IR is to cut off these custom Region-like parts of the model and complete the model with the Region layers where required. Source\n  From the above diagram, it seems lucid why we obtained these two layers as output from the Inference Engine. Pre-conversion to IR, they are named as simply YOLO layers while post-conversion, they are named as YoloRegion.\nNow, we know that we have 2 layers from v3-tiny. From the theory of anchors and grid, we know that both these layers function differently. So, we start with first finding their parameters. The yolov3-tiny.cfg is the source of all these parameters. We just need to pick them from this file manually OR use the .xml and .bin. We have already initialised the net as:\n# Read the IR as a IENetwork self.net = IENetwork(model = model_xml, weights = model_bin) These params are extracted from this net as self.net.layers[layer_name].params. In the demo provided by OpenVINO docs, these params or parameters are hard coded as:\nclass YoloParams: # ------------------------------------------- Extracting layer parameters ------------------------------------------ # Magic numbers are copied from yolov3-tiny.cfg file (Look in the project folder). If the params can\u0026#39;t be extracted automatically, use these hard-coded values. def __init__(self, param, side): self.num = 3 if \u0026#39;num\u0026#39; not in param else int(param[\u0026#39;num\u0026#39;]) self.coords = 4 if \u0026#39;coords\u0026#39; not in param else int(param[\u0026#39;coords\u0026#39;]) self.classes = 80 if \u0026#39;classes\u0026#39; not in param else int(param[\u0026#39;classes\u0026#39;]) self.anchors = [10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0] if \u0026#39;anchors\u0026#39; not in param else [float(a) for a in param[\u0026#39;anchors\u0026#39;].split(\u0026#39;,\u0026#39;)] if \u0026#39;mask\u0026#39; in param: mask = [int(idx) for idx in param[\u0026#39;mask\u0026#39;].split(\u0026#39;,\u0026#39;)] self.num = len(mask) # Collect pairs of anchors to mask/use maskedAnchors = [] for idx in mask: maskedAnchors += [self.anchors[idx * 2], self.anchors[idx * 2 + 1]] self.anchors = maskedAnchors self.side = side # 26 for first layer and 13 for second self.isYoloV3 = \u0026#39;mask\u0026#39; in param # Weak way to determine but the only one. def log_params(self): params_to_print = {\u0026#39;classes\u0026#39;: self.classes, \u0026#39;num\u0026#39;: self.num, \u0026#39;coords\u0026#39;: self.coords, \u0026#39;anchors\u0026#39;: self.anchors} [log.info(\u0026#34; {:8}: {}\u0026#34;.format(param_name, param)) for param_name, param in params_to_print.items()] To understand the mask mentioned here, actually, the .cfg file provides 6 pairs of anchors. These anchors are divided among these 2 feature (output) layers in a pre-determined fashion; the parameter param stores this info. To look into the param attribute of both these feature layers:\n# Layer 1 Params: {'anchors': '10,14,23,27,37,58,81,82,135,169,344,319', 'axis': '1', 'classes': '80', 'coords': '4', 'do_softmax': '0', 'end_axis': '3', 'mask': '0,1,2', 'num': '6'} # Layer 2 Params: {'anchors': '10,14,23,27,37,58,81,82,135,169,344,319', 'axis': '1', 'classes': '80', 'coords': '4', 'do_softmax': '0', 'end_axis': '3', 'mask': '3,4,5', 'num': '6'} The attribute mask helps in distributing/allocating the anchors between the layers. Post-process params or the objects of the class YoloParamslook like:\n# Layer 1 [ INFO ] Layer detector/yolo-v3-tiny/Conv_12/BiasAdd/YoloRegion parameters: [ INFO ] classes : 80 [ INFO ] num : 3 [ INFO ] coords : 4 [ INFO ] anchors : [10.0, 14.0, 23.0, 27.0, 37.0, 58.0] # Layer 2 [ INFO ] Layer detector/yolo-v3-tiny/Conv_9/BiasAdd/YoloRegion parameters: [ INFO ] classes : 80 [ INFO ] num : 3 [ INFO ] coords : 4 [ INFO ] anchors : [81.0, 82.0, 135.0, 169.0, 344.0, 319.0] This log is dumped by log_params in the above class. Another important element in the class definition is self.isYoloV3 = 'mask' in param. This simply helps us to determine whether the model being used is v3 or not. Actually, the mask is exclusive to YOLOv3 and tiny version. Previous versions lack it.\nAfter the output layer has been extracted, we have a 3D array filled with mysteriously packed data that is the treasure we seek. The method used to pack has been discussed in the theory part above. We write a parser function that parses/simplifies this and call it parse_yolo_region(). This function takes in the array full of raw values (let\u0026rsquo;s call it packed array) and gives out list of all detected objects. The function does the following. The two output blobs are (1,255,26,26) and (1,255,13,13). Let it be (1,255,side,side) for this blog (the side attribute is dedicated for this. Look up the definition of the YoloParams class). The side x side represents the grid and the 255 values are the array we showed earlier.\n  One method to decode this array for both the layers is:\nfor oth in range(0, blob.shape[1], 85): # 255 for row in range(blob.shape[2]): # 13 for col in range(blob.shape[3]): # 13 info_per_anchor = blob[0, oth:oth+85, row, col] #print(\u0026#34;prob\u0026#34;+str(prob)) x, y, width, height, prob = info_per_anchor[:5] Next, we find if any of the anchor boxes found an object and if it did, what class was it. There were 80 classes and the one with the highest probablity is the answer.\nif(prob \u0026lt; threshold): continue # Now the remaining terms (l+5:l+85) are 80 Classes class_id = np.argmax(info_per_anchor[5:]) At the threshold confidence of 0.1 or 10%, the classes detected in our test image of the cycle+man, are\nperson prob:0.19843937456607819 person prob:0.7788506746292114 bicycle prob:0.8749380707740784 bicycle prob:0.8752843737602234 The x and y coordinates obtained are relative to the cell. To get the coordinates with respect to the entire image, we add the grid index and finally normalize the result with the side parameter.\nx = (col + x) / params.side y = (row + y) / params.side To relate with above explained example, the commands can be related with the following terms used in the original paper.\nbx = (Cx + x) / params.side by = (Cy + y) / params.side The aspect ratio or width and height, can be a big number or even negative, so we use exponent to correct it.\ntry: width = exp(width) height = exp(height) except OverflowError: continue These values are already normalised. To get absolute values of width and height, we need to multiply them with their respective anchor width or height and again normalize by the image width or height respectively (fixed to 416x416 for v3 and v3-tiny). Why we do this, wait for it\u0026hellip;\nsize_normalizer = (resized_image_w, resized_image_h) if params.isYoloV3 else (params.side, params.side) n = int(oth/85) width = width * params.anchors[2 * n] / size_normalizer[0] height = height * params.anchors[2 * n + 1] / size_normalizer[1] To similarly get absolute coordinates of top-left and bottom right point of the box, we use the xand y values we determined and use the normalised width and height to get the values. w/2 shifts the point from center of the cell to the left boundary and y/2 shifts it to the upper boundary. Together, they give the top-left corner of the box. To resize these bounding boxes to the original image, we scale it up using the dimensions of the image (w_scale=h_scale=416).\nxmin = int((x - w / 2) * w_scale) ymin = int((y - h / 2) * h_scale) xmax = int(xmin + w * w_scale) ymax = int(ymin + h * h_scale) Now, we have the desired observations from the 2 detector layers and we enpack them into objects to get:\nIn Layer detector/yolo-v3-tiny/Conv_12/BiasAdd/YoloRegion Detected Objects {'xmin': 707, 'xmax': 721, 'ymin': 53, 'ymax': 68, 'class_id': 8, 'confidence': 0.0016403508} In Layer detector/yolo-v3-tiny/Conv_9/BiasAdd/YoloRegion Detected Objects {'xmin': 707, 'xmax': 721, 'ymin': 53, 'ymax': 68, 'class_id': 8, 'confidence': 0.0016403508} {'xmin': 257, 'xmax': 454, 'ymin': 32, 'ymax': 323, 'class_id': 0, 'confidence': 0.29021382} {'xmin': 247, 'xmax': 470, 'ymin': 31, 'ymax': 373, 'class_id': 0, 'confidence': 0.34315744} {'xmin': 231, 'xmax': 534, 'ymin': 165, 'ymax': 410, 'class_id': 1, 'confidence': 0.6760541} {'xmin': 232, 'xmax': 540, 'ymin': 188, 'ymax': 428, 'class_id': 1, 'confidence': 0.23595412} But there are too many detections for just a single bicycle and person; this is an inherent issue with YOLO which leads to duplicate predictions beacause it is very likely that two or more anchors of same or different cell detect a particular object with different or even same probablities. If we plot all these boxes on the image, we get\n To remove these duplicate boxes, we employ Non-Maximal Suppression and Intersection over Union.\nNon-Maximal Suppression: Let\u0026rsquo;s not be perplexed with the fancy term. It would have been just fine even if one didn\u0026rsquo;t know it; we are already familiar with it but not the name. It refers to filtering objects on the basis of confidence.\nIntersection over Union (IoU): If we have two bounding boxes, then, IoU is defined as\n  IoU = dividing the area of overlap between the bounding boxes by the area of union [source]\n  It is used for two purposes:\n It helps us benchmark the accuracy of our model predictions. Using it, we can figure out how well does our predicted bounding box overlap with the ground truth bounding box. The higher the IoU, the better the performance. The results can be interpreted as    IoU for performance check\n   It helps us remove duplicate bounding boxes for the same object. Exactly the problem that we are facing with the cyclist test case. For, this, we sort all the predictions/objects in descending order of their confidence. If two bounding boxes are pointing to the same object, their IoU would definitely be very high. In this case, we choose the box with higher confidence (i.e., the first box) and reject the second one. If the IoU is very low, this would possibly mean that the two boxes point to different objects of the same class(like different dogs or different cats in the same picture). We use IoU solely for this purpose.  objects = sorted(objects, key=lambda obj : obj[\u0026#39;confidence\u0026#39;], reverse=True) for i in range(len(objects)): if objects[i][\u0026#39;confidence\u0026#39;] == 0: continue for j in range(i + 1, len(objects)): # We perform IOU on objects of same class only if(objects[i][\u0026#39;class_id\u0026#39;] != objects[j][\u0026#39;class_id\u0026#39;]): continue if intersection_over_union(objects[i], objects[j]) \u0026gt; args.iou_threshold: objects[j][\u0026#39;confidence\u0026#39;] = 0 # Drawing objects with respect to the --prob_threshold CLI parameter objects = [obj for obj in objects if obj[\u0026#39;confidence\u0026#39;] \u0026gt;= args.prob_threshold] print(f\u0026#34;final objects:{objects}\u0026#34;) where intersection_over_union is defined as\ndef intersection_over_union(box_1, box_2): width_of_overlap_area = min(box_1[\u0026#39;xmax\u0026#39;], box_2[\u0026#39;xmax\u0026#39;]) - max(box_1[\u0026#39;xmin\u0026#39;], box_2[\u0026#39;xmin\u0026#39;]) height_of_overlap_area = min(box_1[\u0026#39;ymax\u0026#39;], box_2[\u0026#39;ymax\u0026#39;]) - max(box_1[\u0026#39;ymin\u0026#39;], box_2[\u0026#39;ymin\u0026#39;]) if width_of_overlap_area \u0026lt; 0 or height_of_overlap_area \u0026lt; 0: area_of_overlap = 0 else: area_of_overlap = width_of_overlap_area * height_of_overlap_area box_1_area = (box_1[\u0026#39;ymax\u0026#39;] - box_1[\u0026#39;ymin\u0026#39;]) * (box_1[\u0026#39;xmax\u0026#39;] - box_1[\u0026#39;xmin\u0026#39;]) box_2_area = (box_2[\u0026#39;ymax\u0026#39;] - box_2[\u0026#39;ymin\u0026#39;]) * (box_2[\u0026#39;xmax\u0026#39;] - box_2[\u0026#39;xmin\u0026#39;]) area_of_union = box_1_area + box_2_area - area_of_overlap if area_of_union == 0: return 0 return area_of_overlap / area_of_union Post this, we get filtered objects as\nfinal objects:[{'xmin': 231, 'xmax': 534, 'ymin': 165, 'ymax': 410, 'class_id': 1, 'confidence': 0.6760541}, {'xmin': 247, 'xmax': 470, 'ymin': 31, 'ymax': 373, 'class_id': 0, 'confidence': 0.34315744}]  Now, we have good detections; on drawing bounding boxes, we get the following results at the confidence threshold of 0.1 (10%) and IoU threshold of 0.4 (40%):\n The entire code used here can be found in my GitHub Repo [HERE]. But I also suggest you look into the demo provided by Intel (Link in references).\nI hope this article made sense. Feel free to find discrepencies in the material, I will try my best to correct them and clarify any doubts in it.\n  Sources  OpenVINO YOLO Demo: https://github.com/opencv/open_model_zoo/tree/master/demos/python_demos/object_detection_demo_yolov3_async Cyclist Image Used: https://unsplash.com/photos/Tzz4XrrdPUE Understanding YOLO : https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e  ",
    "ref": "/blog/yolov3_decoding/"
  },{
    "title": "About",
    "date": "February 28, 2019",
    "description": "This is my personal collection of findings and sharable things that I life put before me. ",
    "body": "",
    "ref": "/about/"
  },{
    "title": "Contact",
    "date": "January 1, 0001",
    "description": "",
    "body": "",
    "ref": "/contact/"
  }]
